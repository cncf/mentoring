## Project Ideas

If you are a project maintainer and are considering mentoring during the GSoC 2026 cycle, please, submit your ideas below using the template.

[Google Summer of Code 2026 Announcement](https://groups.google.com/g/google-summer-of-code-discuss/c/D-aU3nHnGBQ/m/VU7lwF_MBQAJ)  
[Google Summer of Code Timeline](https://developers.google.com/open-source/gsoc/timeline)

Key GSoC 2026 dates:
* Organizations application period: Monday, Jan 19, to Tuesday, Feb 3, 2026
* CNCF Project proposals submissions recommendation: Wednesday Jan 14, 2026
  **Note**, proposals can still be submitted after this recommended date, but the Mentorship team needs time to evaluate the proposals and package our application. The more proposals we have, the stronger our org application will be.

You can find the project ideas from previous year [here](./2025.md).

> **NOTE:** Please note that GSoC is a program known for its strict deadlines. In addition to responding to your mentee on time, you will be required to submit evaluations on time. Failures to meet the deadlines might affect CNCF's future participation in GSoC.

---

### Template

```
#### CNCF Project Name

##### Project Title

- Description:
- Expected Outcome:
- Recommended Skills:
- Expected project size: # one of small (~90 hour projects), medium (~175 hour projects) and large (~350 hour projects)
- Mentor(s): #For GSoC, it is **required** to have at least 2 mentors with 1 being a primary mentor.
  - Jane Doe (@jane-github, jane@email.address) - primary
  - John Doe (@john-github, john@email.address)
- Upstream Issue (URL):
```

---

## Ideas


### kgateway

##### Benchmarking and Performance Evaluation of Inference Routing Extensions in kgateway

- Description:
kgateway provides inference routing capabilities based on the Kubernetes Gateway API Inference Extension project. This integration enables advanced behaviors such as model-aware routing, serving priority, and customizable load-balancing of self-hosted Generative AI models.

However, there is currently no standardized or reproducible way to evaluate the performance impact of these inference routing extensions.

This project aims to design and implement a comprehensive benchmarking framework to measure the latency, throughput, and resource overhead introduced by inference routing extensions in kgateway. The benchmarks will help maintainers and users understand performance tradeoffs, validate optimizations, and guide future architectural decisions.

- Expected Outcome:
  - A reproducible benchmarking framework for inference routing extensions in kgateway
  - Benchmark scenarios covering:
    - Baseline gateway routing vs inference-enabled routing
    - Different inference extensions and EPP configurations
    - Request/response and streaming inference workloads
  - Collected metrics including:
    - End-to-end latency (p50 / p95 / p99)
    - Throughput
    - CPU and memory overhead
  - Automated benchmark execution (e.g., via CI or documented scripts)
  - Documentation describing benchmark methodology, results interpretation, and best practices

- Recommended Skills:
  - Go
  - Kubernetes
  - Familiarity with gateways or networking concepts
  - Basic understanding of AI inference workloads is a plus

- Expected project size:
  Medium (~175 hour projects)

- Mentor(s):
  - Primary Mentor: Nina Polshakova (@npolshakova, nina.polshakova@solo.io)
  - Secondary Mentor: Daneyon Hansen (@danehans, daneyon.hansen@solo.io)

- Upstream Issue (URL):
  https://github.com/kgateway-dev/kgateway/issues/12289
